{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e2e716d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:11:59.465496Z",
     "start_time": "2022-11-19T17:11:58.134752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import dataloader\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "batch_sz = 4\n",
    "learn_rate = 1e-3\n",
    "train_num = 1000\n",
    "\n",
    "#导入训练集数据\n",
    "train_data = dataloader.DataLoader(\n",
    "    datasets.CIFAR10(root='data/', train=True, transform=transforms.Compose([\n",
    "        transforms.Resize(size=224),      \n",
    "        transforms.ToTensor(),      \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])         \n",
    "    ]), download=True), shuffle=True, batch_size=batch_sz,num_workers=1\n",
    ")\n",
    "\n",
    "# 导入测试集数据\n",
    "train_test = dataloader.DataLoader(\n",
    "    datasets.CIFAR10(root='data/', train=False, transform=transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]), download=True), shuffle=True, batch_size=batch_sz,num_workers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb33e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9886b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a5f547f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:12:02.313976Z",
     "start_time": "2022-11-19T17:12:00.907415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "First=True\n",
    "for x,label in train_data:\n",
    "    if(First):\n",
    "        print(x.shape)\n",
    "        First=False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7e72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb8004bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:12:04.276819Z",
     "start_time": "2022-11-19T17:12:04.263820Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    #每个stage中维度拓展的倍数\n",
    "    extention=4\n",
    "\n",
    "    #定义初始化的网络和参数\n",
    "    def __init__(self,inplane,midplane,stride,downsample=None):\n",
    "        super(Bottleneck,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(inplane,midplane,kernel_size=1,stride=stride,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(midplane)\n",
    "        self.conv2=nn.Conv2d(midplane,midplane,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(midplane)\n",
    "        self.conv3=nn.Conv2d(midplane,midplane*self.extention,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn3=nn.BatchNorm2d(midplane*self.extention)\n",
    "        self.relu=nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.downsample=downsample\n",
    "        self.stride=stride\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #参差数据\n",
    "        residual=x\n",
    "        \n",
    "        #卷积操作\n",
    "        out=self.relu(self.bn1(self.conv1(x)))\n",
    "        out=self.relu(self.bn2(self.conv2(out)))\n",
    "        out=self.relu(self.bn3(self.conv3(out)))\n",
    "\n",
    "        #是否直连（如果时Identity block就是直连；如果是Conv Block就需要对参差边进行卷积，改变通道数和size）\n",
    "        if(self.downsample!=None):\n",
    "            residual=self.downsample(x)\n",
    "        \n",
    "        #将参差部分和卷积部分相加\n",
    "        out=out+residual\n",
    "        out=self.relu(out)\n",
    "\n",
    "        return out\n",
    "    def make_layer(self,block,midplane,block_num,stride=1):\n",
    "        '''\n",
    "            block:block模块\n",
    "            midplane：每个模块中间运算的维度，一般等于输出维度/4\n",
    "            block_num：重复次数\n",
    "            stride：Conv Block的步长\n",
    "        '''\n",
    "\n",
    "        block_list=[]\n",
    "\n",
    "        #先计算要不要加downsample模块\n",
    "        downsample=None\n",
    "        if(stride!=1 or self.inplane!=midplane*block.extention):\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplane,midplane*block.extention,stride=stride,kernel_size=1,bias=False),\n",
    "                nn.BatchNorm2d(midplane*block.extention)\n",
    "            )\n",
    "\n",
    "\n",
    "        #Conv Block\n",
    "        conv_block=block(self.inplane,midplane,stride=stride,downsample=downsample)\n",
    "        block_list.append(conv_block)\n",
    "        self.inplane=midplane*block.extention\n",
    "\n",
    "        #Identity Block\n",
    "        for i in range(1,block_num):\n",
    "            block_list.append(block(self.inplane,midplane,stride=1))\n",
    "        \n",
    "        return nn.Sequential(*block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adae72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef93ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3e62d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T16:34:20.533629Z",
     "start_time": "2022-11-19T16:34:20.521498Z"
    }
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "512f88d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:12:13.929678Z",
     "start_time": "2022-11-19T17:12:13.907755Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    #初始化网络结构和参数\n",
    "    def __init__(self,block,layers,num_classes=1000):\n",
    "        #self.inplane为当前的fm的通道数\n",
    "        self.inplane=64\n",
    "\n",
    "        super(ResNet,self).__init__()\n",
    "\n",
    "        #参数\n",
    "        self.block=block\n",
    "        self.layers=layers\n",
    "\n",
    "        #stem的网络层\n",
    "        self.conv1=nn.Conv2d(3,self.inplane,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(self.inplane)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,padding=1,stride=2)\n",
    "\n",
    "        #64，128，256，512是指扩大4倍之前的维度,即Identity Block的中间维度\n",
    "        self.stage1=self.make_layer(self.block,64,self.layers[0],stride=1)\n",
    "        self.stage2=self.make_layer(self.block,128,self.layers[1],stride=2)\n",
    "        self.stage3=self.make_layer(self.block,256,self.layers[2],stride=2)\n",
    "        self.stage4=self.make_layer(self.block,512,self.layers[3],stride=2)\n",
    "\n",
    "        #后续的网络\n",
    "        self.avgpool=nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(512 * block.extention, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #stem部分:conv+bn+relu+maxpool\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.maxpool(out)\n",
    "\n",
    "        #block\n",
    "        out=self.stage1(out)\n",
    "        out=self.stage2(out)\n",
    "        out=self.stage3(out)\n",
    "        out=self.stage4(out)\n",
    "\n",
    "        #分类\n",
    "        out=self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out=self.fc(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def make_layer(self,block,midplane,block_num,stride=1):\n",
    "        '''\n",
    "            block:block模块\n",
    "            midplane：每个模块中间运算的维度，一般等于输出维度/4\n",
    "            block_num：重复次数\n",
    "            stride：Conv Block的步长\n",
    "        '''\n",
    "\n",
    "        block_list=[]\n",
    "\n",
    "        #先计算要不要加downsample模块\n",
    "        downsample=None\n",
    "        if(stride!=1 or self.inplane!=midplane*block.extention):\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplane,midplane*block.extention,stride=stride,kernel_size=1,bias=False),\n",
    "                nn.BatchNorm2d(midplane*block.extention)\n",
    "            )\n",
    "\n",
    "\n",
    "        #Conv Block\n",
    "        conv_block=block(self.inplane,midplane,stride=stride,downsample=downsample)\n",
    "        block_list.append(conv_block)\n",
    "        self.inplane=midplane*block.extention\n",
    "\n",
    "        #Identity Block\n",
    "        for i in range(1,block_num):\n",
    "            block_list.append(block(self.inplane,midplane,stride=1))\n",
    "        \n",
    "        return nn.Sequential(*block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd93d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bef1ba68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:12:21.101871Z",
     "start_time": "2022-11-19T17:12:21.006188Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # #Identity Block的操作（Identity Block的dowmsample为None）\n",
    "    # bottle=Bottleneck(256,64,stride=1,downsample=None)\n",
    "    # x=torch.randn(1,256,300,300)\n",
    "    # x=bottle(x)\n",
    "    # print(x.shape)\n",
    "\n",
    "\n",
    "    #Resnet\n",
    "    resnet = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8518e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5118f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T17:12:23.558005Z",
     "start_time": "2022-11-19T17:12:23.459059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.35 GiB already allocated; 0 bytes free; 5.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14344\\1235152622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ch6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\d2l\\torch.py\u001b[0m in \u001b[0;36mtrain_ch6\u001b[1;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training on'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 987\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    983\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    984\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.35 GiB already allocated; 0 bytes free; 5.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "lr, num_epochs, batch_size = 0.01, 10, 32\n",
    "\n",
    "d2l.train_ch6(resnet, train_data,train_test , num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1832e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63dadff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T16:55:42.111579Z",
     "start_time": "2022-11-19T16:55:42.088930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.cuda.is_available())#是否有可用的gpu\n",
    "print(torch.cuda.device_count())#有几个可用的gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"#声明gpu\n",
    "dev=torch.device('cuda:0')#调用哪个gpu\n",
    "a=torch.rand(100,100).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffef6556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T16:56:16.151988Z",
     "start_time": "2022-11-19T16:56:16.139460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "(8, 6) NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "print(torch.cuda.is_available())#是否有可用的gpu\n",
    "print(torch.cuda.device_count())#有几个可用的gpu\n",
    "print(torch.cuda.current_device())#可用gpu编号\n",
    "print( torch.cuda.get_device_capability(device=None),  torch.cuda.get_device_name(device=None))#可用gpu内存大小，可用gpu的名字\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"#声明gpu\n",
    "dev=torch.device('cuda:0')#调用哪个gpu\n",
    "a=torch.rand(100,100).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f35112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
